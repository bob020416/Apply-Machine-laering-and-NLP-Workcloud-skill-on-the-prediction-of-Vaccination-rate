# -*- coding: utf-8 -*-
"""Team 6 __  Vaccine Side  Effects

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g51ZvQ5Syvj-xJeyuPNahY7ZtRrEboNU
"""

from tqdm import tqdm
import datetime


import requests
import pandas as pd
from bs4 import BeautifulSoup
import re
import datetime

url = "https://www.ptt.cc/bbs/Gossiping/search?q=疫苗副作用"
payload = {'from': '/bbs/Gossiping/index.html', 'yes': 'yes'}


posts_data = []
comments_data = []

rs = requests.session()
response = rs.post("https://www.ptt.cc/ask/over18", data=payload)

for _ in range(5):  # Control the number of pages you want to scrape
    response = rs.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    links = soup.find_all("div", class_="title")
    for link in links:
        if link.a is not None:
            page_url = "https://www.ptt.cc/" + link.a["href"]

            response = rs.get(page_url)
            result = BeautifulSoup(response.text, "html.parser")

            main_content = result.find("div", id="main-content")
            article_info = main_content.find_all("span", class_="article-meta-value")

            if len(article_info) != 0:
                post_id = link.a["href"].split('/')[-1].split('.')[1]
                platform = 'PTT'
                post_title = article_info[2].string
                post_date = datetime.datetime.strptime(article_info[3].string, '%a %b %d %H:%M:%S %Y').date()

                posts_data.append({'Post_ID': post_id, 'Platform': platform, 'Post_Title': post_title, 'Post_Date': post_date})

                content = main_content.get_text('\n', strip=True)
                content = content.split('\n--\n')[0]
                content = re.sub('\n\n+', '\n', content)
                content = re.sub('\n+--', '\n', content)

                comments = main_content.find_all('div', class_='push')
                for idx, comment in enumerate(comments):
                  comment_date = comment.find('span', class_='push-ipdatetime').text.strip()
                  comment_date = re.sub(r'\d+\.\d+\.\d+\.\d+ ', '', comment_date)  # Remove IP address
                  comment_date = datetime.datetime.strptime(comment_date, '%m/%d %H:%M')
                  comment_date = comment_date.replace(year=post_date.year).date()
                  comment_text = comment.find('span', class_='push-content').text.strip()[1:]
                  comments_data.append({'Comment_ID': f'{post_id}_{idx}', 'Post_ID': post_id, 'Comment_Text': comment_text, 'Comment_Date': comment_date})

    while True:
      prev_page_link = soup.find("a", string="‹ 上頁")
      if prev_page_link:
          url = "https://www.ptt.cc/" + prev_page_link["href"]
      else:
          break  

posts_df = pd.DataFrame(posts_data)
comments_df = pd.DataFrame(comments_data)

print(posts_df)
print(comments_df)

posts_df = pd.DataFrame(posts_data)
posts_df.to_csv('posts_df.csv', index=False)

comments_df = pd.DataFrame(comments_data)
comments_df.to_csv('comments_df.csv', index=False)



mport pandas as pd
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
!pip install nltk
!pip install -U scikit-learn
nltk.download('snowball_data')
nltk.download('vader_lexicon')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

title_df = pd.read_csv('post - posts_df.csv')
comment_df = pd.read_csv('comment - comments_df(1).csv')

df = pd.merge(df_title, df_comment, on='Post_ID')

# clean title and comment text
from nltk.stem.snowball import SnowballStemmer
stop_words = set(stopwords.words("english"))
stemmer = SnowballStemmer("english")

def clean_text(text):
    words = word_tokenize(text.lower())
    words = [stemmer.stem(w) for w in words if not w in stop_words and w.isalpha()]
    cleaned_text = " ".join(words)
    return cleaned_text

title_df['title_clean'] = df['post_title'].apply(lambda x: clean_text(x))
comment_df['comment_clean'] = df['Comment'].apply(lambda x: clean_text(x))

title_df['title_clean'] = df['post_title'].apply(lambda x: clean_text(x))
comment_df['comment_clean'] = df['Comment'].apply(lambda x: clean_text(x))

# define vaccine names and group comments by vaccine name
vaccine_names = ["BNT","AZ", "Modner"]
comment_dfs = []

for name in vaccine_names:
    # combine comments that mention "Modner" or "Mo"
    if name == "Modner":
        comment_subset = df_comment[df_comment["Comment"].str.contains("Modner|Mo")]
    else:
        comment_subset = df_comment[df_comment["Comment"].str.contains(name)]
    
    # assign name to comments in subset
    comment_subset["vaccine_name"] = name

    print(comment_subset)

# add subset to list of dataframes
comment_dfs.append(comment_subset)

# combine all comments into one dataframe
comment_df = pd.concat(comment_dfs)

# convert text to TF-IDF feature matrix
vectorizer = TfidfVectorizer()
feature_matrix = vectorizer.fit_transform(comment_df["comment_clean"].tolist())

from sklearn.cluster import KMeans
# cluster comments into three groups using KMeans
num_clusters = 3
km = KMeans(n_clusters=num_clusters, max_iter=10000, n_init=50, random_state=42)
km.fit(feature_matrix)

# assign cluster labels to each comment
comment_df['cluster_label'] = pd.Series(km.labels_)

# convert text to TF-IDF feature matrix
vectorizer = TfidfVectorizer()
feature_matrix = vectorizer.fit_transform(comment_df["comment_clean"].tolist())

# cluster comments into three groups using KMeans
num_clusters = 3
km = KMeans(n_clusters=num_clusters, max_iter=10000, n_init=50, random_state=42)
km.fit(feature_matrix)

# assign cluster labels to each comment
comment_df["cluster_label"] = km.labels_


from nltk.sentiment.vader import SentimentIntensityAnalyzer
# perform sentiment analysis on each comment
analyzer = SentimentIntensityAnalyzer()
comment_df["sentiment"] = comment_df["Comment"].apply(lambda x: analyzer.polarity_scores(x)["compound"])

# compute average sentiment score for each cluster and vaccine
grouped = comment_df.groupby(["vaccine_name", "cluster_label"])
sentiments = grouped["sentiment"].mean()

# print sentiment scores
for name in vaccine_names:
    print("Sentiment scores for {}:".format(name))
    for i in range(num_clusters):
        print("Cluster {} sentiment score: {:.2f}".format(i, sentiments[name, i]))



!pip install -U spacy
!python -m spacy download en_core_web_md
!pip install transformers
!pip install xgboost
# or
!pip install lightgbm
!pip install jieba wordcloud matplotlib
!pip install pillow

import pandas as pd
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

nltk.download('punkt')
nltk.download('stopwords')

posts = pd.read_csv('post.csv')
comments = pd.read_csv('comment.csv')

def preprocess(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    words = word_tokenize(text.lower())
    words = [word for word in words if word not in stopwords.words('english')]
    return ' '.join(words)

comments['Comment_Text1'] = comments['Comment'].apply(preprocess)
posts['Post_Title1'] = posts['post_title'].apply(preprocess)

merged_data = comments.merge(posts, on='Post_ID')
grouped_data = merged_data.groupby('Comment_Date').agg({'Comment_Text1': ' '.join, 'Post_Title1': ' '.join}).reset_index()

vaccination_data2 = pd.read_csv('vaccination_data2.csv')

grouped_data['Comment_Date'] = pd.to_datetime(grouped_data['Comment_Date'])
vaccination_data2['Date'] = pd.to_datetime(vaccination_data2['Date'])
final_data = grouped_data.merge(vaccination_data2, left_on='Comment_Date', right_on='Date', how='inner')

# Filter out rows where Brand is "ALL"
final_data = final_data[final_data['Brand'] != "ALL"]

grouped_vaccination_data = final_data.groupby(['Comment_Date', 'Brand']).agg({'Total_increase': 'sum'}).reset_index()
most_popular_brands = grouped_vaccination_data.loc[grouped_vaccination_data.groupby('Comment_Date')['Total_increase'].idxmax()]

grouped_data = grouped_data.merge(most_popular_brands[['Comment_Date', 'Brand']], on='Comment_Date', how='inner')

# Generate X_text after merging with most_popular_brands
vectorizer = TfidfVectorizer()
X_text = vectorizer.fit_transform(grouped_data['Comment_Text1'] + ' ' + grouped_data['Post_Title1'])

X = X_text
y = grouped_data['Brand']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

from sklearn.metrics import confusion_matrix

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)

# Calculate accuracy for each class
class_accuracies = cm.diagonal() / cm.sum(axis=1)

# Create a mapping between class labels and their corresponding indices in the confusion matrix
label_to_index = {label: i for i, label in enumerate(clf.classes_)}
index_to_label = {i: label for label, i in label_to_index.items()}

# Plot the accuracy for each class
n = len(index_to_label)
index = np.arange(n)
width = 0.5

fig, ax = plt.subplots(figsize=(10, 5))
rects = ax.bar(index, class_accuracies, width, label='Accuracy')

ax.set_xlabel('Vaccine Brands')
ax.set_ylabel('Accuracy')
ax.set_title('Accuracy per Vaccine Brand')
ax.set_xticks(index)
ax.set_xticklabels([index_to_label[i] for i in index])
ax.legend()

fig.tight_layout()
plt.show()

import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt

def tokenize_chinese(text):
    stopwords = []
    with open('chinese_stopwords.txt', 'r', encoding='utf-8') as f:
        for line in f:
            stopwords.append(line.strip())
    word_list = jieba.cut(text)
    filtered_words = [word for word in word_list if word not in stopwords]
    return ' '.join(filtered_words)

all_text = ' '.join(comments['Comment_Text'].astype(str)) + ' '.join(posts['Post_Title'].astype(str))

tokenized_text = tokenize_chinese(all_text)

wordcloud = WordCloud(font_path='NotoSansCJKtc-Black.otf',  # Update with the path to a Chinese font file
                      background_color='white',
                      width=800,
                      height=600,
                      max_words=150,
                      collocations=False).generate(tokenized_text)
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()