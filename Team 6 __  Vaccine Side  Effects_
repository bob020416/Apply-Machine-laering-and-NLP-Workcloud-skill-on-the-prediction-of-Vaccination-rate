{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"beYIb2-aT-0S"},"outputs":[],"source":["from tqdm import tqdm\n","import datetime\n","\n","\n","import requests\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import re\n","import datetime\n","\n","url = \"https://www.ptt.cc/bbs/Gossiping/search?q=疫苗副作用\"\n","payload = {'from': '/bbs/Gossiping/index.html', 'yes': 'yes'}\n","\n","\n","posts_data = []\n","comments_data = []\n","\n","rs = requests.session()\n","response = rs.post(\"https://www.ptt.cc/ask/over18\", data=payload)\n","\n","for _ in range(5):  # Control the number of pages you want to scrape\n","    response = rs.get(url)\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","    links = soup.find_all(\"div\", class_=\"title\")\n","    for link in links:\n","        if link.a is not None:\n","            page_url = \"https://www.ptt.cc/\" + link.a[\"href\"]\n","\n","            response = rs.get(page_url)\n","            result = BeautifulSoup(response.text, \"html.parser\")\n","\n","            main_content = result.find(\"div\", id=\"main-content\")\n","            article_info = main_content.find_all(\"span\", class_=\"article-meta-value\")\n","\n","            if len(article_info) != 0:\n","                post_id = link.a[\"href\"].split('/')[-1].split('.')[1]\n","                platform = 'PTT'\n","                post_title = article_info[2].string\n","                post_date = datetime.datetime.strptime(article_info[3].string, '%a %b %d %H:%M:%S %Y').date()\n","\n","                posts_data.append({'Post_ID': post_id, 'Platform': platform, 'Post_Title': post_title, 'Post_Date': post_date})\n","\n","                content = main_content.get_text('\\n', strip=True)\n","                content = content.split('\\n--\\n')[0]\n","                content = re.sub('\\n\\n+', '\\n', content)\n","                content = re.sub('\\n+--', '\\n', content)\n","\n","                comments = main_content.find_all('div', class_='push')\n","                for idx, comment in enumerate(comments):\n","                  comment_date = comment.find('span', class_='push-ipdatetime').text.strip()\n","                  comment_date = re.sub(r'\\d+\\.\\d+\\.\\d+\\.\\d+ ', '', comment_date)  # Remove IP address\n","                  comment_date = datetime.datetime.strptime(comment_date, '%m/%d %H:%M')\n","                  comment_date = comment_date.replace(year=post_date.year).date()\n","                  comment_text = comment.find('span', class_='push-content').text.strip()[1:]\n","                  comments_data.append({'Comment_ID': f'{post_id}_{idx}', 'Post_ID': post_id, 'Comment_Text': comment_text, 'Comment_Date': comment_date})\n","\n","    while True:\n","      prev_page_link = soup.find(\"a\", string=\"‹ 上頁\")\n","      if prev_page_link:\n","          url = \"https://www.ptt.cc/\" + prev_page_link[\"href\"]\n","      else:\n","          break  \n","\n","posts_df = pd.DataFrame(posts_data)\n","comments_df = pd.DataFrame(comments_data)\n","\n","print(posts_df)\n","print(comments_df)"]},{"cell_type":"code","source":["posts_df = pd.DataFrame(posts_data)\n","posts_df.to_csv('posts_df.csv', index=False)\n","\n","comments_df = pd.DataFrame(comments_data)\n","comments_df.to_csv('comments_df.csv', index=False)"],"metadata":{"id":"TD9UqVpPUKWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fy4Ub4SJUNzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mport pandas as pd\n","import re\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","!pip install nltk\n","!pip install -U scikit-learn\n","nltk.download('snowball_data')\n","nltk.download('vader_lexicon')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans"],"metadata":{"id":"7AdK1aQ0UOA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["title_df = pd.read_csv('post - posts_df.csv')\n","comment_df = pd.read_csv('comment - comments_df(1).csv')\n","\n","df = pd.merge(df_title, df_comment, on='Post_ID')"],"metadata":{"id":"c0UYPEXRUgtx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# clean title and comment text\n","from nltk.stem.snowball import SnowballStemmer\n","stop_words = set(stopwords.words(\"english\"))\n","stemmer = SnowballStemmer(\"english\")\n","\n","def clean_text(text):\n","    words = word_tokenize(text.lower())\n","    words = [stemmer.stem(w) for w in words if not w in stop_words and w.isalpha()]\n","    cleaned_text = \" \".join(words)\n","    return cleaned_text\n","\n","title_df['title_clean'] = df['post_title'].apply(lambda x: clean_text(x))\n","comment_df['comment_clean'] = df['Comment'].apply(lambda x: clean_text(x))"],"metadata":{"id":"ZSbcsITiUkkp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["title_df['title_clean'] = df['post_title'].apply(lambda x: clean_text(x))\n","comment_df['comment_clean'] = df['Comment'].apply(lambda x: clean_text(x))"],"metadata":{"id":"ELezbZeTUzuA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define vaccine names and group comments by vaccine name\n","vaccine_names = [\"BNT\",\"AZ\", \"Modner\"]\n","comment_dfs = [] "],"metadata":{"id":"MZlTDjsqU07A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for name in vaccine_names:\n","    # combine comments that mention \"Modner\" or \"Mo\"\n","    if name == \"Modner\":\n","        comment_subset = df_comment[df_comment[\"Comment\"].str.contains(\"Modner|Mo\")]\n","    else:\n","        comment_subset = df_comment[df_comment[\"Comment\"].str.contains(name)]\n","    \n","    # assign name to comments in subset\n","    comment_subset[\"vaccine_name\"] = name\n","\n","    print(comment_subset)"],"metadata":{"id":"EKoyTUftU-Ul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add subset to list of dataframes\n","comment_dfs.append(comment_subset)\n","\n","# combine all comments into one dataframe\n","comment_df = pd.concat(comment_dfs)\n","\n","# convert text to TF-IDF feature matrix\n","vectorizer = TfidfVectorizer()\n","feature_matrix = vectorizer.fit_transform(comment_df[\"comment_clean\"].tolist())"],"metadata":{"id":"MOKU9edbU_TI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","# cluster comments into three groups using KMeans\n","num_clusters = 3\n","km = KMeans(n_clusters=num_clusters, max_iter=10000, n_init=50, random_state=42)\n","km.fit(feature_matrix)\n","\n","# assign cluster labels to each comment\n","comment_df['cluster_label'] = pd.Series(km.labels_)"],"metadata":{"id":"JvVQjTmIVDQn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert text to TF-IDF feature matrix\n","vectorizer = TfidfVectorizer()\n","feature_matrix = vectorizer.fit_transform(comment_df[\"comment_clean\"].tolist())\n","\n","# cluster comments into three groups using KMeans\n","num_clusters = 3\n","km = KMeans(n_clusters=num_clusters, max_iter=10000, n_init=50, random_state=42)\n","km.fit(feature_matrix)\n","\n","# assign cluster labels to each comment\n","comment_df[\"cluster_label\"] = km.labels_\n","\n","\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","# perform sentiment analysis on each comment\n","analyzer = SentimentIntensityAnalyzer()\n","comment_df[\"sentiment\"] = comment_df[\"Comment\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])"],"metadata":{"id":"7lmsiPHgVHNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute average sentiment score for each cluster and vaccine\n","grouped = comment_df.groupby([\"vaccine_name\", \"cluster_label\"])\n","sentiments = grouped[\"sentiment\"].mean()"],"metadata":{"id":"eu4dqnxdVKse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print sentiment scores\n","for name in vaccine_names:\n","    print(\"Sentiment scores for {}:\".format(name))\n","    for i in range(num_clusters):\n","        print(\"Cluster {} sentiment score: {:.2f}\".format(i, sentiments[name, i]))"],"metadata":{"id":"deqlYCNRVNwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XkOKA6VaVQeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U spacy\n","!python -m spacy download en_core_web_md\n","!pip install transformers\n","!pip install xgboost\n","# or\n","!pip install lightgbm\n","!pip install jieba wordcloud matplotlib\n","!pip install pillow"],"metadata":{"id":"wKogsGoQVQmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","posts = pd.read_csv('post.csv')\n","comments = pd.read_csv('comment.csv')\n","\n","def preprocess(text):\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    words = word_tokenize(text.lower())\n","    words = [word for word in words if word not in stopwords.words('english')]\n","    return ' '.join(words)\n","\n","comments['Comment_Text1'] = comments['Comment'].apply(preprocess)\n","posts['Post_Title1'] = posts['post_title'].apply(preprocess)\n","\n","merged_data = comments.merge(posts, on='Post_ID')\n","grouped_data = merged_data.groupby('Comment_Date').agg({'Comment_Text1': ' '.join, 'Post_Title1': ' '.join}).reset_index()\n","\n","vaccination_data2 = pd.read_csv('vaccination_data2.csv')\n","\n","grouped_data['Comment_Date'] = pd.to_datetime(grouped_data['Comment_Date'])\n","vaccination_data2['Date'] = pd.to_datetime(vaccination_data2['Date'])\n","final_data = grouped_data.merge(vaccination_data2, left_on='Comment_Date', right_on='Date', how='inner')\n","\n","# Filter out rows where Brand is \"ALL\"\n","final_data = final_data[final_data['Brand'] != \"ALL\"]\n","\n","grouped_vaccination_data = final_data.groupby(['Comment_Date', 'Brand']).agg({'Total_increase': 'sum'}).reset_index()\n","most_popular_brands = grouped_vaccination_data.loc[grouped_vaccination_data.groupby('Comment_Date')['Total_increase'].idxmax()]\n","\n","grouped_data = grouped_data.merge(most_popular_brands[['Comment_Date', 'Brand']], on='Comment_Date', how='inner')\n","\n","# Generate X_text after merging with most_popular_brands\n","vectorizer = TfidfVectorizer()\n","X_text = vectorizer.fit_transform(grouped_data['Comment_Text1'] + ' ' + grouped_data['Post_Title1'])\n","\n","X = X_text\n","y = grouped_data['Brand']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","clf.fit(X_train, y_train)\n","\n","y_pred = clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Classification Report:\\n\", report)\n"],"metadata":{"id":"qcya2o4PVUXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","# Calculate confusion matrix\n","cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n","\n","# Calculate accuracy for each class\n","class_accuracies = cm.diagonal() / cm.sum(axis=1)\n","\n","# Create a mapping between class labels and their corresponding indices in the confusion matrix\n","label_to_index = {label: i for i, label in enumerate(clf.classes_)}\n","index_to_label = {i: label for label, i in label_to_index.items()}\n","\n","# Plot the accuracy for each class\n","n = len(index_to_label)\n","index = np.arange(n)\n","width = 0.5\n","\n","fig, ax = plt.subplots(figsize=(10, 5))\n","rects = ax.bar(index, class_accuracies, width, label='Accuracy')\n","\n","ax.set_xlabel('Vaccine Brands')\n","ax.set_ylabel('Accuracy')\n","ax.set_title('Accuracy per Vaccine Brand')\n","ax.set_xticks(index)\n","ax.set_xticklabels([index_to_label[i] for i in index])\n","ax.legend()\n","\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"G6f0jiu9VXcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import jieba\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","def tokenize_chinese(text):\n","    stopwords = []\n","    with open('chinese_stopwords.txt', 'r', encoding='utf-8') as f:\n","        for line in f:\n","            stopwords.append(line.strip())\n","    word_list = jieba.cut(text)\n","    filtered_words = [word for word in word_list if word not in stopwords]\n","    return ' '.join(filtered_words)\n","\n","all_text = ' '.join(comments['Comment_Text'].astype(str)) + ' '.join(posts['Post_Title'].astype(str))\n","\n","tokenized_text = tokenize_chinese(all_text)\n","\n","wordcloud = WordCloud(font_path='NotoSansCJKtc-Black.otf',  # Update with the path to a Chinese font file\n","                      background_color='white',\n","                      width=800,\n","                      height=600,\n","                      max_words=150,\n","                      collocations=False).generate(tokenized_text)\n","plt.figure(figsize=(10, 8))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"xQC_WcXaVbPn"},"execution_count":null,"outputs":[]}]}